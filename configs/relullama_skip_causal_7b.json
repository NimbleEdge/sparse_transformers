{
    "_name_or_path": "SparseLLM/ReluLLaMA-7B",
    "sparsity_method": "naive",
    "sparsities": [
        0.743192738853395,
        0.7336088079027832,
        0.6890966654755175,
        0.7145767104811966,
        0.73568778578192,
        0.7505242507904768,
        0.7502159993164241,
        0.747797972522676,
        0.7135628159157932,
        0.7085652560926974,
        0.6838056156411767,
        0.6900686351582408,
        0.6819221628829837,
        0.6771378242410719,
        0.6827241298742592,
        0.6764037436805665,
        0.6925274380482733,
        0.7076996429823339,
        0.7007281989790499,
        0.6772957886569202,
        0.6694492469541728,
        0.6437577940523624,
        0.6286926441825926,
        0.611934173386544,
        0.604402104858309,
        0.607235555537045,
        0.6056556575931609,
        0.6122667123563588,
        0.6011746157892048,
        0.5901505807414651,
        0.592667915392667,
        0.5540782236494124
    ],
    "architectures": ["LlamaSkipConnectionForCausalLM"], 
    "bos_token_id": 1, 
    "eos_token_id": 2, 
    "hidden_act": "relu",
    "hidden_size": 4096, 
    "initializer_range": 0.02, 
    "intermediate_size": 11008, 
    "max_length": 4096, 
    "max_position_embeddings": 2048, 
    "model_type": "llama-skip", 
    "num_attention_heads": 32, 
    "num_hidden_layers": 32, 
    "num_key_value_heads": 32, 
    "pad_token_id": 0, 
    "pretraining_tp": 1, 
    "rms_norm_eps": 1e-05, 
    "rope_scaling": null, 
    "tie_word_embeddings": false, 
    "torch_dtype": "float32", 
    "transformers_version": "4.31.0", 
    "use_cache": true, 
    "vocab_size": 32000
}