{
    "_name_or_path": "SparseLLM/ReluLLaMA-7B",
      "sparsity_method": "naive",
    "sparsities": [
        74.3192738853395,
        73.36088079027832,
        68.90966654755175,
        71.45767104811966,
        73.568778578192,
        75.05242507904768,
        75.02159993164241,
        74.7797972522676,
        71.35628159157932,
        70.85652560926974,
        68.38056156411767,
        69.00686351582408,
        68.19221628829837,
        67.7137824241072,
        68.27241298742592,
        67.64037436805665,
        69.25274380482733,
        70.76996429823339,
        70.07281989790499,
        67.72957886569202,
        66.94492469541728,
        64.37577940523624,
        62.86926441825926,
        61.1934173386544,
        60.4402104858309,
        60.7235555537045,
        60.56556575931609,
        61.22667123563588,
        60.117461578920484,
        59.01505807414651,
        59.266791539266706,
        55.40782236494124
    ],
    "architectures": ["LlamaSkipConnectionForCausalLM"], 
    "bos_token_id": 1, 
    "eos_token_id": 2, 
    "hidden_act": "relu",
    "hidden_size": 4096, 
    "initializer_range": 0.02, 
    "intermediate_size": 11008, 
    "max_length": 4096, 
    "max_position_embeddings": 2048, 
    "model_type": "llama-skip", 
    "num_attention_heads": 32, 
    "num_hidden_layers": 32, 
    "num_key_value_heads": 32, 
    "pad_token_id": 0, 
    "pretraining_tp": 1, 
    "rms_norm_eps": 1e-05, 
    "rope_scaling": null, 
    "tie_word_embeddings": false, 
    "torch_dtype": "float32", 
    "transformers_version": "4.31.0", 
    "use_cache": true, 
    "vocab_size": 32000
}