Some weights of LlamaSkipConnectionForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.0.mlp_lora_proj.down.weight', 'model.layers.0.mlp_lora_proj.intermediate', 'model.layers.0.mlp_lora_proj.output', 'model.layers.0.mlp_lora_proj.up.weight', 'model.layers.0.mlp_mask', 'model.layers.1.mlp_lora_proj.down.weight', 'model.layers.1.mlp_lora_proj.intermediate', 'model.layers.1.mlp_lora_proj.output', 'model.layers.1.mlp_lora_proj.up.weight', 'model.layers.1.mlp_mask', 'model.layers.10.mlp_lora_proj.down.weight', 'model.layers.10.mlp_lora_proj.intermediate', 'model.layers.10.mlp_lora_proj.output', 'model.layers.10.mlp_lora_proj.up.weight', 'model.layers.10.mlp_mask', 'model.layers.11.mlp_lora_proj.down.weight', 'model.layers.11.mlp_lora_proj.intermediate', 'model.layers.11.mlp_lora_proj.output', 'model.layers.11.mlp_lora_proj.up.weight', 'model.layers.11.mlp_mask', 'model.layers.12.mlp_lora_proj.down.weight', 'model.layers.12.mlp_lora_proj.intermediate', 'model.layers.12.mlp_lora_proj.output', 'model.layers.12.mlp_lora_proj.up.weight', 'model.layers.12.mlp_mask', 'model.layers.13.mlp_lora_proj.down.weight', 'model.layers.13.mlp_lora_proj.intermediate', 'model.layers.13.mlp_lora_proj.output', 'model.layers.13.mlp_lora_proj.up.weight', 'model.layers.13.mlp_mask', 'model.layers.14.mlp_lora_proj.down.weight', 'model.layers.14.mlp_lora_proj.intermediate', 'model.layers.14.mlp_lora_proj.output', 'model.layers.14.mlp_lora_proj.up.weight', 'model.layers.14.mlp_mask', 'model.layers.15.mlp_lora_proj.down.weight', 'model.layers.15.mlp_lora_proj.intermediate', 'model.layers.15.mlp_lora_proj.output', 'model.layers.15.mlp_lora_proj.up.weight', 'model.layers.15.mlp_mask', 'model.layers.2.mlp_lora_proj.down.weight', 'model.layers.2.mlp_lora_proj.intermediate', 'model.layers.2.mlp_lora_proj.output', 'model.layers.2.mlp_lora_proj.up.weight', 'model.layers.2.mlp_mask', 'model.layers.3.mlp_lora_proj.down.weight', 'model.layers.3.mlp_lora_proj.intermediate', 'model.layers.3.mlp_lora_proj.output', 'model.layers.3.mlp_lora_proj.up.weight', 'model.layers.3.mlp_mask', 'model.layers.4.mlp_lora_proj.down.weight', 'model.layers.4.mlp_lora_proj.intermediate', 'model.layers.4.mlp_lora_proj.output', 'model.layers.4.mlp_lora_proj.up.weight', 'model.layers.4.mlp_mask', 'model.layers.5.mlp_lora_proj.down.weight', 'model.layers.5.mlp_lora_proj.intermediate', 'model.layers.5.mlp_lora_proj.output', 'model.layers.5.mlp_lora_proj.up.weight', 'model.layers.5.mlp_mask', 'model.layers.6.mlp_lora_proj.down.weight', 'model.layers.6.mlp_lora_proj.intermediate', 'model.layers.6.mlp_lora_proj.output', 'model.layers.6.mlp_lora_proj.up.weight', 'model.layers.6.mlp_mask', 'model.layers.7.mlp_lora_proj.down.weight', 'model.layers.7.mlp_lora_proj.intermediate', 'model.layers.7.mlp_lora_proj.output', 'model.layers.7.mlp_lora_proj.up.weight', 'model.layers.7.mlp_mask', 'model.layers.8.mlp_lora_proj.down.weight', 'model.layers.8.mlp_lora_proj.intermediate', 'model.layers.8.mlp_lora_proj.output', 'model.layers.8.mlp_lora_proj.up.weight', 'model.layers.8.mlp_mask', 'model.layers.9.mlp_lora_proj.down.weight', 'model.layers.9.mlp_lora_proj.intermediate', 'model.layers.9.mlp_lora_proj.output', 'model.layers.9.mlp_lora_proj.up.weight', 'model.layers.9.mlp_mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Configuring for 8 CPU threads
Model device: cpu
Input IDs device: cpu
Attention Mask device: cpu
Running CPU inference benchmarks...
--------------------------------------------------
Warming up models...

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Global Component Timing Statistics:

attention:
  Avg: 4.660ms
  Min: 3.931ms
  Max: 11.093ms
  Total: 149.132ms
  Count: 32

lora_proj:
  Avg: 1.924ms
  Min: 0.116ms
  Max: 45.574ms
  Total: 61.580ms
  Count: 32

compute_weights:
  Avg: 27.501ms
  Min: 2.843ms
  Max: 45.120ms
  Total: 880.024ms
  Count: 32

mlp:
  Avg: 9.933ms
  Min: 0.913ms
  Max: 16.953ms
  Total: 317.850ms
  Count: 32

layernorm:
  Avg: 0.184ms
  Min: 0.135ms
  Max: 0.421ms
  Total: 5.884ms
  Count: 32

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Global Component Timing Statistics:

attention:
  Avg: 32.843ms
  Min: 3.825ms
  Max: 220.825ms
  Total: 5254.942ms
  Count: 160

lora_proj:
  Avg: 3.049ms
  Min: 0.109ms
  Max: 32.836ms
  Total: 487.848ms
  Count: 160

compute_weights:
  Avg: 28.174ms
  Min: 3.064ms
  Max: 62.477ms
  Total: 4507.830ms
  Count: 160

mlp:
  Avg: 11.497ms
  Min: 0.909ms
  Max: 42.308ms
  Total: 1839.548ms
  Count: 160

layernorm:
  Avg: 0.174ms
  Min: 0.002ms
  Max: 0.238ms
  Total: 27.893ms
  Count: 160

SkipLLaMA Scripted CPU Results:
Average time: 1.501s
Min time: 0.831s
Max time: 4.439s
Individual times: ['0.831s', '4.439s', '2.754s', '0.846s', '0.835s', '0.840s', '1.245s', '0.872s', '0.843s']
