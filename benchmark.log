Some weights of LlamaSkipConnectionForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.0.mlp.combined_proj_buffer', 'model.layers.0.mlp.down_proj_buffer', 'model.layers.0.mlp_lora_proj.down.weight', 'model.layers.0.mlp_lora_proj.intermediate', 'model.layers.0.mlp_lora_proj.output', 'model.layers.0.mlp_lora_proj.up.weight', 'model.layers.0.mlp_mask', 'model.layers.1.mlp.combined_proj_buffer', 'model.layers.1.mlp.down_proj_buffer', 'model.layers.1.mlp_lora_proj.down.weight', 'model.layers.1.mlp_lora_proj.intermediate', 'model.layers.1.mlp_lora_proj.output', 'model.layers.1.mlp_lora_proj.up.weight', 'model.layers.1.mlp_mask', 'model.layers.10.mlp.combined_proj_buffer', 'model.layers.10.mlp.down_proj_buffer', 'model.layers.10.mlp_lora_proj.down.weight', 'model.layers.10.mlp_lora_proj.intermediate', 'model.layers.10.mlp_lora_proj.output', 'model.layers.10.mlp_lora_proj.up.weight', 'model.layers.10.mlp_mask', 'model.layers.11.mlp.combined_proj_buffer', 'model.layers.11.mlp.down_proj_buffer', 'model.layers.11.mlp_lora_proj.down.weight', 'model.layers.11.mlp_lora_proj.intermediate', 'model.layers.11.mlp_lora_proj.output', 'model.layers.11.mlp_lora_proj.up.weight', 'model.layers.11.mlp_mask', 'model.layers.12.mlp.combined_proj_buffer', 'model.layers.12.mlp.down_proj_buffer', 'model.layers.12.mlp_lora_proj.down.weight', 'model.layers.12.mlp_lora_proj.intermediate', 'model.layers.12.mlp_lora_proj.output', 'model.layers.12.mlp_lora_proj.up.weight', 'model.layers.12.mlp_mask', 'model.layers.13.mlp.combined_proj_buffer', 'model.layers.13.mlp.down_proj_buffer', 'model.layers.13.mlp_lora_proj.down.weight', 'model.layers.13.mlp_lora_proj.intermediate', 'model.layers.13.mlp_lora_proj.output', 'model.layers.13.mlp_lora_proj.up.weight', 'model.layers.13.mlp_mask', 'model.layers.14.mlp.combined_proj_buffer', 'model.layers.14.mlp.down_proj_buffer', 'model.layers.14.mlp_lora_proj.down.weight', 'model.layers.14.mlp_lora_proj.intermediate', 'model.layers.14.mlp_lora_proj.output', 'model.layers.14.mlp_lora_proj.up.weight', 'model.layers.14.mlp_mask', 'model.layers.15.mlp.combined_proj_buffer', 'model.layers.15.mlp.down_proj_buffer', 'model.layers.15.mlp_lora_proj.down.weight', 'model.layers.15.mlp_lora_proj.intermediate', 'model.layers.15.mlp_lora_proj.output', 'model.layers.15.mlp_lora_proj.up.weight', 'model.layers.15.mlp_mask', 'model.layers.2.mlp.combined_proj_buffer', 'model.layers.2.mlp.down_proj_buffer', 'model.layers.2.mlp_lora_proj.down.weight', 'model.layers.2.mlp_lora_proj.intermediate', 'model.layers.2.mlp_lora_proj.output', 'model.layers.2.mlp_lora_proj.up.weight', 'model.layers.2.mlp_mask', 'model.layers.3.mlp.combined_proj_buffer', 'model.layers.3.mlp.down_proj_buffer', 'model.layers.3.mlp_lora_proj.down.weight', 'model.layers.3.mlp_lora_proj.intermediate', 'model.layers.3.mlp_lora_proj.output', 'model.layers.3.mlp_lora_proj.up.weight', 'model.layers.3.mlp_mask', 'model.layers.4.mlp.combined_proj_buffer', 'model.layers.4.mlp.down_proj_buffer', 'model.layers.4.mlp_lora_proj.down.weight', 'model.layers.4.mlp_lora_proj.intermediate', 'model.layers.4.mlp_lora_proj.output', 'model.layers.4.mlp_lora_proj.up.weight', 'model.layers.4.mlp_mask', 'model.layers.5.mlp.combined_proj_buffer', 'model.layers.5.mlp.down_proj_buffer', 'model.layers.5.mlp_lora_proj.down.weight', 'model.layers.5.mlp_lora_proj.intermediate', 'model.layers.5.mlp_lora_proj.output', 'model.layers.5.mlp_lora_proj.up.weight', 'model.layers.5.mlp_mask', 'model.layers.6.mlp.combined_proj_buffer', 'model.layers.6.mlp.down_proj_buffer', 'model.layers.6.mlp_lora_proj.down.weight', 'model.layers.6.mlp_lora_proj.intermediate', 'model.layers.6.mlp_lora_proj.output', 'model.layers.6.mlp_lora_proj.up.weight', 'model.layers.6.mlp_mask', 'model.layers.7.mlp.combined_proj_buffer', 'model.layers.7.mlp.down_proj_buffer', 'model.layers.7.mlp_lora_proj.down.weight', 'model.layers.7.mlp_lora_proj.intermediate', 'model.layers.7.mlp_lora_proj.output', 'model.layers.7.mlp_lora_proj.up.weight', 'model.layers.7.mlp_mask', 'model.layers.8.mlp.combined_proj_buffer', 'model.layers.8.mlp.down_proj_buffer', 'model.layers.8.mlp_lora_proj.down.weight', 'model.layers.8.mlp_lora_proj.intermediate', 'model.layers.8.mlp_lora_proj.output', 'model.layers.8.mlp_lora_proj.up.weight', 'model.layers.8.mlp_mask', 'model.layers.9.mlp.combined_proj_buffer', 'model.layers.9.mlp.down_proj_buffer', 'model.layers.9.mlp_lora_proj.down.weight', 'model.layers.9.mlp_lora_proj.intermediate', 'model.layers.9.mlp_lora_proj.output', 'model.layers.9.mlp_lora_proj.up.weight', 'model.layers.9.mlp_mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cpu
Device set to use cpu
Configuring for 8 CPU threads
Using device: cpu
Model device: cpu
Input IDs device: cpu
Attention Mask device: cpu

Running CPU inference benchmarks...
--------------------------------------------------
Warming up models...

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float32

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float32

SkipLLaMA Scripted MLP CPU Results:
Average time: 0.008s
Min time: 0.008s
Max time: 0.009s
Individual times: ['0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s']

Standard LLaMA MLP CPU Results:
Average time: 0.029s
Min time: 0.019s
Max time: 0.200s
Individual times: ['0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.105s', '0.200s', '0.176s', '0.043s', '0.043s', '0.167s', '0.196s', '0.052s', '0.029s', '0.124s', '0.026s', '0.053s', '0.200s', '0.106s', '0.043s', '0.192s', '0.043s', '0.089s', '0.019s', '0.020s', '0.019s', '0.019s', '0.020s', '0.020s', '0.019s', '0.020s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.019s', '0.019s', '0.020s', '0.019s', '0.020s', '0.020s', '0.020s', '0.020s', '0.020s', '0.019s', '0.020s', '0.019s', '0.019s', '0.020s', '0.020s', '0.019s', '0.019s', '0.019s', '0.020s', '0.020s', '0.020s', '0.020s', '0.020s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.020s', '0.020s', '0.019s', '0.019s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.020s', '0.019s', '0.020s', '0.019s', '0.020s', '0.019s', '0.020s', '0.019s', '0.019s', '0.019s', '0.020s', '0.020s', '0.019s', '0.020s', '0.020s', '0.020s', '0.019s', '0.020s', '0.019s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.019s', '0.019s', '0.032s', '0.020s', '0.020s', '0.019s', '0.020s', '0.019s', '0.020s', '0.020s', '0.027s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.021s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.020s', '0.020s', '0.020s', '0.019s', '0.019s', '0.019s', '0.020s', '0.041s', '0.019s', '0.020s', '0.019s', '0.020s', '0.019s', '0.020s', '0.020s', '0.020s', '0.020s', '0.020s', '0.019s']

SkipLLaMA Scripted CPU Results:
Average time: 0.711s
Min time: 0.707s
Max time: 0.720s
Individual times: ['0.713s', '0.709s', '0.709s', '0.710s', '0.711s', '0.711s', '0.707s', '0.708s', '0.720s']

Standard LLaMA CPU Results:
Average time: 1.426s
Min time: 1.160s
Max time: 3.122s
Individual times: ['3.122s', '1.163s', '1.173s', '1.160s', '1.165s', '1.172s', '1.194s', '1.166s', '1.517s']

CPU Speedups:
Scripted vs Standard: 2.01x
