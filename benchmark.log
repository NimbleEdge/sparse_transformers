Some weights of LlamaSkipConnectionForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.0.mlp_lora_proj.down.weight', 'model.layers.0.mlp_lora_proj.intermediate', 'model.layers.0.mlp_lora_proj.output', 'model.layers.0.mlp_lora_proj.up.weight', 'model.layers.0.mlp_mask', 'model.layers.1.mlp_lora_proj.down.weight', 'model.layers.1.mlp_lora_proj.intermediate', 'model.layers.1.mlp_lora_proj.output', 'model.layers.1.mlp_lora_proj.up.weight', 'model.layers.1.mlp_mask', 'model.layers.10.mlp_lora_proj.down.weight', 'model.layers.10.mlp_lora_proj.intermediate', 'model.layers.10.mlp_lora_proj.output', 'model.layers.10.mlp_lora_proj.up.weight', 'model.layers.10.mlp_mask', 'model.layers.11.mlp_lora_proj.down.weight', 'model.layers.11.mlp_lora_proj.intermediate', 'model.layers.11.mlp_lora_proj.output', 'model.layers.11.mlp_lora_proj.up.weight', 'model.layers.11.mlp_mask', 'model.layers.12.mlp_lora_proj.down.weight', 'model.layers.12.mlp_lora_proj.intermediate', 'model.layers.12.mlp_lora_proj.output', 'model.layers.12.mlp_lora_proj.up.weight', 'model.layers.12.mlp_mask', 'model.layers.13.mlp_lora_proj.down.weight', 'model.layers.13.mlp_lora_proj.intermediate', 'model.layers.13.mlp_lora_proj.output', 'model.layers.13.mlp_lora_proj.up.weight', 'model.layers.13.mlp_mask', 'model.layers.14.mlp_lora_proj.down.weight', 'model.layers.14.mlp_lora_proj.intermediate', 'model.layers.14.mlp_lora_proj.output', 'model.layers.14.mlp_lora_proj.up.weight', 'model.layers.14.mlp_mask', 'model.layers.15.mlp_lora_proj.down.weight', 'model.layers.15.mlp_lora_proj.intermediate', 'model.layers.15.mlp_lora_proj.output', 'model.layers.15.mlp_lora_proj.up.weight', 'model.layers.15.mlp_mask', 'model.layers.2.mlp_lora_proj.down.weight', 'model.layers.2.mlp_lora_proj.intermediate', 'model.layers.2.mlp_lora_proj.output', 'model.layers.2.mlp_lora_proj.up.weight', 'model.layers.2.mlp_mask', 'model.layers.3.mlp_lora_proj.down.weight', 'model.layers.3.mlp_lora_proj.intermediate', 'model.layers.3.mlp_lora_proj.output', 'model.layers.3.mlp_lora_proj.up.weight', 'model.layers.3.mlp_mask', 'model.layers.4.mlp_lora_proj.down.weight', 'model.layers.4.mlp_lora_proj.intermediate', 'model.layers.4.mlp_lora_proj.output', 'model.layers.4.mlp_lora_proj.up.weight', 'model.layers.4.mlp_mask', 'model.layers.5.mlp_lora_proj.down.weight', 'model.layers.5.mlp_lora_proj.intermediate', 'model.layers.5.mlp_lora_proj.output', 'model.layers.5.mlp_lora_proj.up.weight', 'model.layers.5.mlp_mask', 'model.layers.6.mlp_lora_proj.down.weight', 'model.layers.6.mlp_lora_proj.intermediate', 'model.layers.6.mlp_lora_proj.output', 'model.layers.6.mlp_lora_proj.up.weight', 'model.layers.6.mlp_mask', 'model.layers.7.mlp_lora_proj.down.weight', 'model.layers.7.mlp_lora_proj.intermediate', 'model.layers.7.mlp_lora_proj.output', 'model.layers.7.mlp_lora_proj.up.weight', 'model.layers.7.mlp_mask', 'model.layers.8.mlp_lora_proj.down.weight', 'model.layers.8.mlp_lora_proj.intermediate', 'model.layers.8.mlp_lora_proj.output', 'model.layers.8.mlp_lora_proj.up.weight', 'model.layers.8.mlp_mask', 'model.layers.9.mlp_lora_proj.down.weight', 'model.layers.9.mlp_lora_proj.intermediate', 'model.layers.9.mlp_lora_proj.output', 'model.layers.9.mlp_lora_proj.up.weight', 'model.layers.9.mlp_mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Configuring for 8 CPU threads
Model device: cpu
Input IDs device: cpu
Attention Mask device: cpu
Running CPU inference benchmarks...
--------------------------------------------------
Warming up models...

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float32

Global Component Timing Statistics:

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Global Component Timing Statistics:

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Global Component Timing Statistics:

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float32

Global Component Timing Statistics:

SkipLLaMA Scripted CPU Results:
Average time: 0.575s
Min time: 0.345s
Max time: 3.658s
Individual times: ['0.348s', '0.357s', '0.366s', '0.356s', '0.345s', '0.354s', '0.353s', '0.401s', '0.358s', '0.780s', '3.658s', '1.819s', '0.383s', '0.383s', '0.385s', '0.382s', '0.389s', '0.379s', '0.382s', '0.382s', '0.385s', '0.385s', '0.386s', '0.388s', '0.388s', '0.387s', '0.386s', '0.390s', '0.388s', '0.382s', '0.390s', '0.386s', '0.385s', '0.388s', '0.388s', '0.395s', '0.780s', '1.309s', '2.126s', '0.664s', '1.618s', '0.359s', '0.366s', '0.366s', '0.369s', '0.372s', '0.368s', '0.365s', '0.364s']

Standard LLaMA CPU Results:
Average time: 0.622s
Min time: 0.375s
Max time: 3.377s
Individual times: ['0.433s', '0.437s', '0.434s', '0.438s', '0.436s', '0.436s', '0.433s', '0.437s', '0.432s', '0.436s', '0.435s', '0.434s', '0.434s', '0.475s', '3.377s', '2.587s', '0.375s', '0.375s', '0.378s', '0.379s', '0.381s', '0.377s', '0.379s', '0.378s', '0.387s', '0.384s', '0.379s', '0.388s', '0.379s', '0.379s', '0.380s', '0.377s', '0.377s', '0.378s', '0.381s', '0.380s', '0.377s', '0.382s', '0.426s', '0.435s', '1.483s', '2.981s', '1.714s', '0.501s', '0.500s', '0.511s', '0.504s', '0.500s', '0.500s']

CPU Speedups:
Scripted vs Standard: 1.08x
