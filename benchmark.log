Some weights of LlamaSkipConnectionForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.0.mlp.combined_proj_buffer', 'model.layers.0.mlp.down_proj_buffer', 'model.layers.0.mlp_lora_proj.down.weight', 'model.layers.0.mlp_lora_proj.intermediate', 'model.layers.0.mlp_lora_proj.output', 'model.layers.0.mlp_lora_proj.up.weight', 'model.layers.0.mlp_mask', 'model.layers.1.mlp.combined_proj_buffer', 'model.layers.1.mlp.down_proj_buffer', 'model.layers.1.mlp_lora_proj.down.weight', 'model.layers.1.mlp_lora_proj.intermediate', 'model.layers.1.mlp_lora_proj.output', 'model.layers.1.mlp_lora_proj.up.weight', 'model.layers.1.mlp_mask', 'model.layers.10.mlp.combined_proj_buffer', 'model.layers.10.mlp.down_proj_buffer', 'model.layers.10.mlp_lora_proj.down.weight', 'model.layers.10.mlp_lora_proj.intermediate', 'model.layers.10.mlp_lora_proj.output', 'model.layers.10.mlp_lora_proj.up.weight', 'model.layers.10.mlp_mask', 'model.layers.11.mlp.combined_proj_buffer', 'model.layers.11.mlp.down_proj_buffer', 'model.layers.11.mlp_lora_proj.down.weight', 'model.layers.11.mlp_lora_proj.intermediate', 'model.layers.11.mlp_lora_proj.output', 'model.layers.11.mlp_lora_proj.up.weight', 'model.layers.11.mlp_mask', 'model.layers.12.mlp.combined_proj_buffer', 'model.layers.12.mlp.down_proj_buffer', 'model.layers.12.mlp_lora_proj.down.weight', 'model.layers.12.mlp_lora_proj.intermediate', 'model.layers.12.mlp_lora_proj.output', 'model.layers.12.mlp_lora_proj.up.weight', 'model.layers.12.mlp_mask', 'model.layers.13.mlp.combined_proj_buffer', 'model.layers.13.mlp.down_proj_buffer', 'model.layers.13.mlp_lora_proj.down.weight', 'model.layers.13.mlp_lora_proj.intermediate', 'model.layers.13.mlp_lora_proj.output', 'model.layers.13.mlp_lora_proj.up.weight', 'model.layers.13.mlp_mask', 'model.layers.14.mlp.combined_proj_buffer', 'model.layers.14.mlp.down_proj_buffer', 'model.layers.14.mlp_lora_proj.down.weight', 'model.layers.14.mlp_lora_proj.intermediate', 'model.layers.14.mlp_lora_proj.output', 'model.layers.14.mlp_lora_proj.up.weight', 'model.layers.14.mlp_mask', 'model.layers.15.mlp.combined_proj_buffer', 'model.layers.15.mlp.down_proj_buffer', 'model.layers.15.mlp_lora_proj.down.weight', 'model.layers.15.mlp_lora_proj.intermediate', 'model.layers.15.mlp_lora_proj.output', 'model.layers.15.mlp_lora_proj.up.weight', 'model.layers.15.mlp_mask', 'model.layers.2.mlp.combined_proj_buffer', 'model.layers.2.mlp.down_proj_buffer', 'model.layers.2.mlp_lora_proj.down.weight', 'model.layers.2.mlp_lora_proj.intermediate', 'model.layers.2.mlp_lora_proj.output', 'model.layers.2.mlp_lora_proj.up.weight', 'model.layers.2.mlp_mask', 'model.layers.3.mlp.combined_proj_buffer', 'model.layers.3.mlp.down_proj_buffer', 'model.layers.3.mlp_lora_proj.down.weight', 'model.layers.3.mlp_lora_proj.intermediate', 'model.layers.3.mlp_lora_proj.output', 'model.layers.3.mlp_lora_proj.up.weight', 'model.layers.3.mlp_mask', 'model.layers.4.mlp.combined_proj_buffer', 'model.layers.4.mlp.down_proj_buffer', 'model.layers.4.mlp_lora_proj.down.weight', 'model.layers.4.mlp_lora_proj.intermediate', 'model.layers.4.mlp_lora_proj.output', 'model.layers.4.mlp_lora_proj.up.weight', 'model.layers.4.mlp_mask', 'model.layers.5.mlp.combined_proj_buffer', 'model.layers.5.mlp.down_proj_buffer', 'model.layers.5.mlp_lora_proj.down.weight', 'model.layers.5.mlp_lora_proj.intermediate', 'model.layers.5.mlp_lora_proj.output', 'model.layers.5.mlp_lora_proj.up.weight', 'model.layers.5.mlp_mask', 'model.layers.6.mlp.combined_proj_buffer', 'model.layers.6.mlp.down_proj_buffer', 'model.layers.6.mlp_lora_proj.down.weight', 'model.layers.6.mlp_lora_proj.intermediate', 'model.layers.6.mlp_lora_proj.output', 'model.layers.6.mlp_lora_proj.up.weight', 'model.layers.6.mlp_mask', 'model.layers.7.mlp.combined_proj_buffer', 'model.layers.7.mlp.down_proj_buffer', 'model.layers.7.mlp_lora_proj.down.weight', 'model.layers.7.mlp_lora_proj.intermediate', 'model.layers.7.mlp_lora_proj.output', 'model.layers.7.mlp_lora_proj.up.weight', 'model.layers.7.mlp_mask', 'model.layers.8.mlp.combined_proj_buffer', 'model.layers.8.mlp.down_proj_buffer', 'model.layers.8.mlp_lora_proj.down.weight', 'model.layers.8.mlp_lora_proj.intermediate', 'model.layers.8.mlp_lora_proj.output', 'model.layers.8.mlp_lora_proj.up.weight', 'model.layers.8.mlp_mask', 'model.layers.9.mlp.combined_proj_buffer', 'model.layers.9.mlp.down_proj_buffer', 'model.layers.9.mlp_lora_proj.down.weight', 'model.layers.9.mlp_lora_proj.intermediate', 'model.layers.9.mlp_lora_proj.output', 'model.layers.9.mlp_lora_proj.up.weight', 'model.layers.9.mlp_mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cpu
Device set to use cpu
Configuring for 8 CPU threads
Using device: cpu
Model device: cpu
Input IDs device: cpu
Attention Mask device: cpu

Running CPU inference benchmarks...
--------------------------------------------------
Warming up models...

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float32

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float32

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float32

SkipLLaMA Scripted MLP CPU Results:
Average time: 0.009s
Min time: 0.008s
Max time: 0.032s
Individual times: ['0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.016s', '0.011s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.028s', '0.008s', '0.008s', '0.010s', '0.010s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.014s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.024s', '0.024s', '0.032s', '0.032s', '0.013s', '0.032s', '0.032s', '0.024s', '0.012s', '0.013s', '0.013s', '0.013s', '0.013s', '0.013s', '0.013s', '0.013s', '0.028s', '0.010s', '0.009s', '0.018s', '0.028s', '0.028s', '0.018s', '0.016s', '0.008s', '0.009s', '0.018s', '0.025s', '0.013s', '0.024s', '0.009s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.009s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.009s', '0.008s', '0.009s', '0.008s', '0.008s', '0.009s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.008s', '0.008s', '0.008s', '0.009s', '0.009s', '0.008s', '0.009s', '0.009s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.009s', '0.008s', '0.008s', '0.009s', '0.009s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.008s', '0.009s', '0.009s', '0.008s', '0.009s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.009s', '0.008s', '0.008s', '0.008s', '0.009s', '0.009s', '0.009s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.008s', '0.009s', '0.008s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.009s', '0.008s', '0.009s', '0.008s', '0.008s', '0.009s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s', '0.008s']

Standard LLaMA MLP CPU Results:
Average time: 0.032s
Min time: 0.016s
Max time: 0.216s
Individual times: ['0.017s', '0.016s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.017s', '0.018s', '0.017s', '0.017s', '0.020s', '0.020s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.019s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.019s', '0.019s', '0.019s', '0.019s', '0.019s', '0.020s', '0.020s', '0.020s', '0.020s', '0.020s', '0.021s', '0.173s', '0.204s', '0.193s', '0.132s', '0.105s', '0.146s', '0.206s', '0.038s', '0.149s', '0.038s', '0.180s', '0.200s', '0.192s', '0.043s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.024s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.025s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.185s', '0.144s', '0.042s', '0.212s', '0.125s', '0.204s', '0.042s', '0.043s', '0.032s', '0.119s', '0.023s', '0.147s', '0.216s', '0.044s', '0.108s', '0.068s', '0.076s', '0.045s', '0.035s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.023s', '0.024s', '0.023s', '0.023s', '0.023s', '0.025s', '0.023s']

SkipLLaMA Scripted CPU Results:
Average time: 0.971s
Min time: 0.666s
Max time: 3.656s
Individual times: ['0.750s', '0.694s', '0.706s', '0.697s', '0.701s', '0.709s', '0.704s', '3.656s', '3.111s', '0.666s', '0.666s', '0.671s', '0.671s', '0.672s', '0.668s', '0.670s', '0.678s', '0.682s', '0.670s']

Standard LLaMA CPU Results:
Average time: 1.943s
Min time: 1.155s
Max time: 7.197s
Individual times: ['1.155s', '1.196s', '7.197s', '1.370s', '1.381s', '1.370s', '1.368s', '1.373s', '1.383s', '1.365s', '1.367s', '5.727s', '2.435s', '1.373s', '1.377s', '1.371s', '1.378s', '1.360s', '1.377s']

CPU Speedups:
Scripted vs Standard: 2.00x
