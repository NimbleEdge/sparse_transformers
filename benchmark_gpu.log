Device set to use cuda
Device set to use cuda
Configuring for 8 CPU threads
Using device: cuda

Running CUDA inference benchmarks...
--------------------------------------------------
Warming up models...

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float16
Model path: meta-llama/Llama-3.2-1B-Instruct

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float16
Model path: meta-llama/Llama-3.2-1B-Instruct

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model dtype: torch.float16
Model path: meta-llama/Llama-3.2-1B-Instruct

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model dtype: torch.float16
Model path: meta-llama/Llama-3.2-1B-Instruct

SkipLLaMA Scripted CUDA Results:
Average time: 0.021s
Min time: 0.021s
Max time: 0.021s
Individual times: ['0.021s', '0.021s', '0.021s', '0.021s', '0.021s', '0.021s', '0.021s', '0.021s', '0.021s', '0.021s']

Standard LLaMA CUDA Results:
Average time: 0.018s
Min time: 0.017s
Max time: 0.018s
Individual times: ['0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.017s', '0.018s']

CUDA Speedups:
Scripted vs Standard: 0.85x
