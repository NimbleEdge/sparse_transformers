Device set to use cuda
Device set to use cuda
Configuring for 8 CPU threads

System Configuration:
--------------------------------------------------
OS: Linux 5.15.0-1079-azure
CPU: x86_64
Physical cores: 8
Total cores: 8
Max CPU frequency: 0MHz
Current CPU frequency: 2544MHz
RAM: Total=54.92GB, Available=51.71GB (5.8% used)

GPU Configuration:
--------------------------------------------------

GPU 0: Tesla T4
Compute capability: 7.5
Total memory: 15.56GB
Free memory: 15.37GB
Multi processors: 40

PyTorch version: 2.5.1
CUDA version: 12.4
--------------------------------------------------
Number of available GPUs: 1
Using devices: cuda, cuda, cuda

Running CUDA inference benchmarks...
--------------------------------------------------
Warming up models...

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model device: cuda
Model path: meta-llama/Llama-3.2-1B-Instruct

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model device: cuda
Model path: meta-llama/Llama-3.2-1B-Instruct

Model type: <class 'src.models.modelling_llama_skip.LlamaSkipConnectionForCausalLM'>
Model device: cuda
Model path: meta-llama/Llama-3.2-1B-Instruct

Model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Model device: cuda
Model path: meta-llama/Llama-3.2-1B-Instruct

SkipLLaMA Scripted CUDA Results:
Average time: 0.021s
Min time: 0.020s
Max time: 0.021s
Individual times: ['0.021s', '0.021s', '0.021s', '0.021s', '0.021s', '0.020s', '0.021s', '0.021s', '0.020s', '0.021s']

Standard LLaMA CUDA Results:
Average time: 0.018s
Min time: 0.018s
Max time: 0.018s
Individual times: ['0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s', '0.018s']

CUDA Speedups:
Scripted vs Standard: 0.86x
