{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/azureuser/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/azureuser/.cache/torch_extensions/py39_cu124/sparse_mlp/build.ninja...\n",
      "/home/azureuser/miniconda3/envs/wsc_env/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module sparse_mlp...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module sparse_mlp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaSkipConnectionForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.0.mlp.lora_gate_proj.0.weight', 'model.layers.0.mlp.lora_gate_proj.1.weight', 'model.layers.1.mlp.lora_gate_proj.0.weight', 'model.layers.1.mlp.lora_gate_proj.1.weight', 'model.layers.10.mlp.lora_gate_proj.0.weight', 'model.layers.10.mlp.lora_gate_proj.1.weight', 'model.layers.11.mlp.lora_gate_proj.0.weight', 'model.layers.11.mlp.lora_gate_proj.1.weight', 'model.layers.12.mlp.lora_gate_proj.0.weight', 'model.layers.12.mlp.lora_gate_proj.1.weight', 'model.layers.13.mlp.lora_gate_proj.0.weight', 'model.layers.13.mlp.lora_gate_proj.1.weight', 'model.layers.14.mlp.lora_gate_proj.0.weight', 'model.layers.14.mlp.lora_gate_proj.1.weight', 'model.layers.15.mlp.lora_gate_proj.0.weight', 'model.layers.15.mlp.lora_gate_proj.1.weight', 'model.layers.2.mlp.lora_gate_proj.0.weight', 'model.layers.2.mlp.lora_gate_proj.1.weight', 'model.layers.3.mlp.lora_gate_proj.0.weight', 'model.layers.3.mlp.lora_gate_proj.1.weight', 'model.layers.4.mlp.lora_gate_proj.0.weight', 'model.layers.4.mlp.lora_gate_proj.1.weight', 'model.layers.5.mlp.lora_gate_proj.0.weight', 'model.layers.5.mlp.lora_gate_proj.1.weight', 'model.layers.6.mlp.lora_gate_proj.0.weight', 'model.layers.6.mlp.lora_gate_proj.1.weight', 'model.layers.7.mlp.lora_gate_proj.0.weight', 'model.layers.7.mlp.lora_gate_proj.1.weight', 'model.layers.8.mlp.lora_gate_proj.0.weight', 'model.layers.8.mlp.lora_gate_proj.1.weight', 'model.layers.9.mlp.lora_gate_proj.0.weight', 'model.layers.9.mlp.lora_gate_proj.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from src.models.modelling_llama_skip import LlamaSkipConnectionForCausalLM\n",
    "from src.models.configuration_llama_skip import LlamaSkipConnectionConfig\n",
    "from transformers.models.llama import LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Register the custom model and config\n",
    "AutoConfig.register(\"llama-skip\", LlamaSkipConnectionConfig)\n",
    "AutoModelForCausalLM.register(LlamaSkipConnectionConfig, LlamaSkipConnectionForCausalLM)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model_id = \"vkkhare/llama-skip\"\n",
    "checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,  trust_remote_code=True)\n",
    "\n",
    "# Create custom config and model\n",
    "config = LlamaSkipConnectionConfig.from_pretrained(model_id)\n",
    "\n",
    "# Load model without device_map\n",
    "model = LlamaSkipConnectionForCausalLM.from_pretrained(\n",
    "    checkpoint, \n",
    "    config=config,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    ").to(device)\n",
    "\n",
    "# Move all masks to the correct device\n",
    "for module in model.modules():\n",
    "    if hasattr(module, 'mask'):\n",
    "        module.mask = module.mask.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8a1e9cadbb4bd8a928aba099a38960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb34e12c665f432ba5d4093462e7f11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3af83197654af4aa449f79f87434c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/vkkhare/llama-skip/commit/390f414e73e4f7631af1f1b41410040aa46d2187', commit_message='Upload config', commit_description='', oid='390f414e73e4f7631af1f1b41410040aa46d2187', pr_url=None, repo_url=RepoUrl('https://huggingface.co/vkkhare/llama-skip', endpoint='https://huggingface.co', repo_type='model', repo_id='vkkhare/llama-skip'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"vkkhare/llama-skip\")\n",
    "config.push_to_hub(\"vkkhare/llama-skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaSkipConnectionForCausalLM(\n",
       "  (model): LlamaSkipConnectionModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaSkipDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaSkipMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "          (lora_gate_proj): Sequential(\n",
       "            (0): Linear(in_features=2048, out_features=1638, bias=False)\n",
       "            (1): Linear(in_features=1638, out_features=8192, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text\n",
    "sequence = \"Give recipe of burrito including all the ingredients and their quantity.\"\n",
    "inputs = tokenizer(\n",
    "    sequence, \n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Explicitly move all input tensors to the same device as model\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Debug prints\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Input IDs device: {input_ids.device}\")\n",
    "print(f\"Attention Mask device: {attention_mask.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 1000,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "out = pipe.model.generate(input[\"input_ids\"], max_length=20)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Give recipe of burrito including all the ingredients and their quantity. \\nHere are the ingredients required'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardPipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=checkpoint,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 1000,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# max_length needs to be set as by default it is 20\n",
    "out = standardPipe.model.generate(input[\"input_ids\"], max_length=20)\n",
    "tokenizer.decode(skip_special_tokens=True, token_ids=out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 inferences.\n",
      "Standard pipeline time:  14.608644485473633\n",
      "Pipeline time:  7.280247211456299\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start1 = time.time()\n",
    "for i in range(15):\n",
    "    out = standardPipe.model.forward(input[\"input_ids\"], use_cache=False)\n",
    "\n",
    "start2 = time.time()\n",
    "print(\"Time taken for 100 inferences.\")\n",
    "print(\"Standard pipeline time: \", start2 - start1)\n",
    "for i in range(15):\n",
    "    out = pipe.model.forward(input[\"input_ids\"],use_cache=False)\n",
    "\n",
    "\n",
    "start3 = time.time()\n",
    "\n",
    "print(\"Pipeline time: \", start3 - start2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b31ade328db4ca3baae409eef50cb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX custom configs for: decoder_model, decoder_with_past_model\n",
      "Submodels to export: model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to export a custom model, but could not find as many custom ONNX configs as the number of submodels to export. Please specifiy the fn_get_submodels argument, that should return a dictionary of submodules with as many items as the provided custom_export_configs dictionary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m onnx_config_with_past \u001b[38;5;241m=\u001b[39m LlamaOnnxConfig(config, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_past\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m custom_onnx_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: onnx_config,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_with_past_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: onnx_config_with_past,\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmain_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmpt_onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation-with-past\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsc_env/lib/python3.9/site-packages/optimum/exporters/onnx/__main__.py:373\u001b[0m, in \u001b[0;36mmain_export\u001b[0;34m(model_name_or_path, output, task, opset, device, dtype, fp16, optimize, monolith, no_post_process, framework, atol, cache_dir, trust_remote_code, pad_token_id, subfolder, revision, force_download, local_files_only, use_auth_token, token, for_ort, do_validation, model_kwargs, custom_onnx_configs, fn_get_submodels, use_subprocess, _variant, library_name, legacy, no_dynamic_axes, do_constant_folding, **kwargs_shapes)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# The preprocessors are loaded as they may be useful to export the model. Notably, some of the static input shapes may be stored in the\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# preprocessors config.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m preprocessors \u001b[38;5;241m=\u001b[39m maybe_load_preprocessors(\n\u001b[1;32m    370\u001b[0m     model_name_or_path, subfolder\u001b[38;5;241m=\u001b[39msubfolder, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 373\u001b[0m \u001b[43monnx_export_from_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonolith\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonolith\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_get_submodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_get_submodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_dynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_subprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_subprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsc_env/lib/python3.9/site-packages/optimum/exporters/onnx/convert.py:1100\u001b[0m, in \u001b[0;36monnx_export_from_model\u001b[0;34m(model, output, opset, optimize, monolith, no_post_process, atol, do_validation, model_kwargs, custom_onnx_configs, fn_get_submodels, _variant, legacy, preprocessors, device, no_dynamic_axes, task, use_subprocess, do_constant_folding, **kwargs_shapes)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1092\u001b[0m         model_type \u001b[38;5;129;01min\u001b[39;00m MODEL_TO_PATCH_FOR_PAST\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m kwargs_shapes\u001b[38;5;241m.\u001b[39mget(input_name) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1095\u001b[0m     ):\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1097\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExporting with a sequence length of 1 a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model is not supported and can yield unexpected results.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m         )\n\u001b[0;32m-> 1100\u001b[0m onnx_config, models_and_onnx_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_submodels_and_onnx_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonolith\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonolith\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_architecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_architecture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_get_submodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_get_submodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m library_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffusers\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Ensure the requested opset is sufficient\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/wsc_env/lib/python3.9/site-packages/optimum/exporters/onnx/utils.py:205\u001b[0m, in \u001b[0;36m_get_submodels_and_onnx_configs\u001b[0;34m(model, task, monolith, custom_onnx_configs, custom_architecture, _variant, library_name, int_dtype, float_dtype, fn_get_submodels, preprocessors, legacy, model_kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_submodels_and_onnx_configs\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    192\u001b[0m     task: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m     model_kwargs: Optional[Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    204\u001b[0m ):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_submodels_and_export_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonolith\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_onnx_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_architecture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_variant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mint_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_get_submodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsc_env/lib/python3.9/site-packages/optimum/exporters/utils.py:610\u001b[0m, in \u001b[0;36m_get_submodels_and_export_configs\u001b[0;34m(model, task, monolith, custom_export_configs, custom_architecture, _variant, library_name, int_dtype, float_dtype, fn_get_submodels, preprocessors, legacy, model_kwargs, exporter)\u001b[0m\n\u001b[1;32m    608\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexporter\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m custom configs for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(custom_export_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    609\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmodels to export: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(submodels_for_export\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to export a custom model, but could not find as many custom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexporter\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m configs as the number of submodels to export. Please specifiy the fn_get_submodels argument, that should return a dictionary of submodules with as many items as the provided custom_export_configs dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, custom_export_config \u001b[38;5;129;01min\u001b[39;00m custom_export_configs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    615\u001b[0m     models_and_export_configs[key] \u001b[38;5;241m=\u001b[39m (submodels_for_export[key], custom_export_config)\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to export a custom model, but could not find as many custom ONNX configs as the number of submodels to export. Please specifiy the fn_get_submodels argument, that should return a dictionary of submodules with as many items as the provided custom_export_configs dictionary."
     ]
    }
   ],
   "source": [
    "from src.models.configuration_llama_skip import LlamaOnnxConfig\n",
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "onnx_config = LlamaOnnxConfig(\n",
    "    config=config,\n",
    "    task=\"text-generation\",\n",
    "    use_past_in_inputs=False\n",
    ")\n",
    "onnx_config_with_past = LlamaOnnxConfig(config, task=\"text-generation\", use_past=True)\n",
    "\n",
    "def get_submodels(model):\n",
    "    return {\n",
    "        \"decoder_model\": model.model,\n",
    "        \"decoder_with_past_model\": model.model\n",
    "    }\n",
    "\n",
    "custom_onnx_configs = {\n",
    "    \"decoder_model\": onnx_config,\n",
    "    \"decoder_with_past_model\": onnx_config_with_past,\n",
    "}\n",
    "\n",
    "main_export(\n",
    "    model_id,\n",
    "    output=\"mpt_onnx\", \n",
    "    task=\"text-generation-with-past\",\n",
    "    trust_remote_code=True,\n",
    "    custom_onnx_configs=custom_onnx_configs,\n",
    "    fn_get_submodels=get_submodels,\n",
    "    no_post_process=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install executorch==0.5.0.dev20241016+cpu --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "import torch\n",
    "from torch.export import export\n",
    "from executorch.exir import to_edge, EdgeCompileConfig, to_edge_transform_and_lower\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.runtime import Runtime\n",
    "\n",
    "# Get the operators supported by executorch\n",
    "runtime = Runtime.get()\n",
    "model = llamaSkipModel.eval() # turn into evaluation mode\n",
    "\n",
    "exported_graph = torch.export.export(model.eval(), args=(input[\"input_ids\"], input[\"attention_mask\"]), strict=False) # Core Aten graph\n",
    "print(\"torch.export.export done\")\n",
    "# Using compile_config=EdgeCompileConfig(_check_ir_validity=False) moves to next but then fails therat edge.to_backend()\n",
    "edge_delegated = to_edge_transform_and_lower(exported_graph, partitioner=[XnnpackPartitioner()]) # Edge Dialect -- Failing here with torch._ops.aten.sym_constrain_range_for_size.default is not Aten Canonical\n",
    "print(\"to_edge_transform_and_lower done\")\n",
    "executorch_program = edge_delegated.to_executorch() # ExecuTorch program\n",
    "print(\"to_executorch done\")\n",
    "pte_path = \"/home/azureuser/weight_caching/checkpoints/llamaskipmodel/llama_skip_model.pte\"\n",
    "\n",
    "with open(pte_path, \"wb\") as file:\n",
    "    executorch_program.write_to_file(file) # Serializing into .pte file\n",
    "print(\"File created\")\n",
    "program = runtime.load_program(pte_path)\n",
    "print(\"load_program successfull\")\n",
    "method = program.load_method(\"forward\")\n",
    "print(\"load_method done\")\n",
    "output = method.execute([input])\n",
    "print(\"method.execute done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(pipe.model, \"skipllama.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model.save_pretrained(\"onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362d3dc82a1b405e848298e11c38ef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f226b053f26481c969ab6fd1a74b041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb87c4413c64ff3901710c3cf9d00ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/vkkhare/llama-skip/commit/23c1b3fdcf17cdb28e0752dd890672427a481eca', commit_message='Upload LlamaSkipConnectionForCausalLM', commit_description='', oid='23c1b3fdcf17cdb28e0752dd890672427a481eca', pr_url=None, repo_url=RepoUrl('https://huggingface.co/vkkhare/llama-skip', endpoint='https://huggingface.co', repo_type='model', repo_id='vkkhare/llama-skip'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.push_to_hub(\"llama-skip\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.save_pretrained(\"onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens/second benchmarking options\n",
    "# 1. llama-bench\n",
    "# 2. tiktoken\n",
    "# 3. nvidia genai-perf\n",
    "\n",
    "# Time taken for 15 inferences.\n",
    "# Standard pipeline time:  14.072068691253662\n",
    "# Pipeline time:  5.465141773223877 (2.5X)\n",
    "\n",
    "# Time taken for 100 inferences.\n",
    "# Standard pipeline time:  67.88692212104797\n",
    "# Pipeline time:  56.4792857170105 (1.2X)\n",
    "\n",
    "# Time taken for 1000 inferences.\n",
    "# Standard pipeline time:  694.2122750282288\n",
    "# Pipeline time:  545.006334066391 (1.2X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
